{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "def model(t_u, w, b):\n",
    "    return w * t_u + b\n",
    "\n",
    "def loss_fn(t_p, t_c):\n",
    "    squared_diffs = (t_p - t_c)**2\n",
    "    return squared_diffs.mean()\n",
    "\n",
    "torch.set_printoptions(sci_mode=False)\n",
    "\n",
    "t_c = torch.tensor([0.5,  14.0, 15.0, 28.0, 11.0,  8.0,  3.0, -4.0,  6.0, 13.0, 21.0])\n",
    "t_u = torch.tensor([35.7, 55.9, 58.2, 81.9, 56.3, 48.9, 33.9, 21.8, 48.4, 60.4, 68.4])\n",
    "\n",
    "# w = torch.ones((), requires_grad=True)\n",
    "# b = torch.zeros((), requires_grad=True)\n",
    "\n",
    "# t_p = model(t_u, w, b)\n",
    "# loss = loss_fn(t_p, t_c)\n",
    "# loss.backward()\n",
    "# w.grad, b.grad\n",
    "\n",
    "# delta = 0.1\n",
    "# learning_rate = 1e-2\n",
    "\n",
    "# loss_rate_of_change_w = \\\n",
    "#     (loss_fn(model(t_u, w + delta, b), t_c) -\n",
    "#      loss_fn(model(t_u, w - delta, b), t_c)) / (2.0 * delta)\n",
    "# w = w - learning_rate * loss_rate_of_change_w\n",
    "\n",
    "# loss_rate_of_change_b = \\\n",
    "#     (loss_fn(model(t_u, w, b + delta), t_c) -\n",
    "#      loss_fn(model(t_u, w, b - delta), t_c)) / (2.0 * delta)\n",
    "# b = b - learning_rate * loss_rate_of_change_b\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 500, Loss 7.860115\n",
      "Epoch 1000, Loss 3.828538\n",
      "Epoch 1500, Loss 3.092191\n",
      "Epoch 2000, Loss 2.957698\n",
      "Epoch 2500, Loss 2.933134\n",
      "Epoch 3000, Loss 2.928648\n",
      "Epoch 3500, Loss 2.927830\n",
      "Epoch 4000, Loss 2.927679\n",
      "Epoch 4500, Loss 2.927652\n",
      "Epoch 5000, Loss 2.927647\n",
      "tensor([  5.3671, -17.3012], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# def model(t_u, w, b):\n",
    "#     return w * t_u + b\n",
    "\n",
    "# def loss_fn(t_p, t_c):\n",
    "#     squared_diffs = (t_p - t_c)**2\n",
    "#     return squared_diffs.mean()\n",
    "\n",
    "t_un =  t_u * 0.1\n",
    "def training_loop(n_epochs, learning_rate, params, t_u, t_c):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        if params.grad is not None:\n",
    "            params.grad.zero_()\n",
    "        \n",
    "        t_p = model(t_u, *params)\n",
    "        loss = loss_fn(t_p, t_c)\n",
    "        loss.backward()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            params -= learning_rate * params.grad\n",
    "\n",
    "        if epoch % 500 == 0:\n",
    "            print('Epoch %d, Loss %f' % (epoch, float(loss)))\n",
    "    return params\n",
    "\n",
    "# torch.set_printoptions(sci_mode=False)\n",
    "\n",
    "# t_c = torch.tensor([0.5,  14.0, 15.0, 28.0, 11.0,  8.0,  3.0, -4.0,  6.0, 13.0, 21.0])\n",
    "# t_u = torch.tensor([35.7, 55.9, 58.2, 81.9, 56.3, 48.9, 33.9, 21.8, 48.4, 60.4, 68.4])\n",
    "\n",
    "learning_rate = 1e-2\n",
    "params = torch.tensor([1.0, 0.0], requires_grad=True)\n",
    "params = training_loop(n_epochs = 5000, learning_rate = learning_rate, params = params, t_u = t_un, t_c = t_c)\n",
    "print(params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 500, Loss: 24.946460723876953\n",
      "Epoch 1000, Loss: 15.752771377563477\n",
      "Epoch 1500, Loss: 9.455153465270996\n",
      "Epoch 2000, Loss: 5.767541885375977\n",
      "Epoch 2500, Loss: 3.932838201522827\n",
      "Epoch 3000, Loss: 3.196812391281128\n",
      "Epoch 3500, Loss: 2.9771506786346436\n",
      "Epoch 4000, Loss: 2.933181047439575\n",
      "Epoch 4500, Loss: 2.9279677867889404\n",
      "Epoch 5000, Loss: 2.9276540279388428\n",
      "tensor([  5.3660, -17.2952], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "def training_loop1(n_epochs, optimizer, params, t_u, t_c):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        t_p = model(t_u, *params)\n",
    "        loss = loss_fn(t_p, t_c)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch % 500 == 0:\n",
    "            print('Epoch {}, Loss: {}'.format(epoch, loss))\n",
    "    \n",
    "    return params\n",
    "\n",
    "params = torch.tensor([1.0, 0.0], requires_grad=True)\n",
    "learning_rate = 1e-2\n",
    "optimizer = optim.SGD([params], lr = learning_rate)\n",
    "params = training_loop1(n_epochs=5000, optimizer=optimizer, params=params, t_u=t_un, t_c=t_c)\n",
    "print(params)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 500, Training loss: 7.161751747131348, Validation loss: 2.25825834274292\n",
      "Epoch 1000, Training loss: 3.470465898513794, Validation loss: 2.71073579788208\n",
      "Epoch 1500, Training loss: 3.0020039081573486, Validation loss: 2.90449595451355\n",
      "Epoch 2000, Training loss: 2.942551612854004, Validation loss: 2.977656602859497\n",
      "Epoch 2500, Training loss: 2.935004472732544, Validation loss: 3.0042498111724854\n",
      "Epoch 3000, Training loss: 2.9340462684631348, Validation loss: 3.0137853622436523\n",
      "Epoch 3500, Training loss: 2.933924674987793, Validation loss: 3.0171902179718018\n",
      "Epoch 4000, Training loss: 2.933910369873047, Validation loss: 3.0184054374694824\n",
      "Epoch 4500, Training loss: 2.933908224105835, Validation loss: 3.018829822540283\n",
      "Epoch 5000, Training loss: 2.933908700942993, Validation loss: 3.0189974308013916\n",
      "tensor([  5.4164, -17.4319], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "def training_loop2(n_epochs, optimizer, params, train_t_u, val_t_u, trina_t_c, val_t_c):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        train_t_p = model(train_t_u, *params)\n",
    "        train_loss =loss_fn(train_t_p, train_t_c)\n",
    "        \n",
    "        val_t_p = model(val_t_u, *params)\n",
    "        val_loss = loss_fn(val_t_p, val_t_c)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch % 500 == 0:\n",
    "            print('Epoch {}, Training loss: {}, Validation loss: {}'.format(epoch, train_loss, val_loss))\n",
    "\n",
    "    return params\n",
    "\n",
    "n_samples = t_u.shape[0]\n",
    "n_val = int(0.2 * n_samples)\n",
    "\n",
    "shuffled_indices = torch.randperm(n_samples)\n",
    "\n",
    "train_indices = shuffled_indices[:-n_val]\n",
    "val_indices = shuffled_indices[-n_val:]\n",
    "\n",
    "train_t_u = t_u[train_indices]\n",
    "train_t_c = t_c[train_indices]\n",
    "\n",
    "val_t_u = t_u[val_indices]\n",
    "val_t_c = t_c[val_indices]\n",
    "\n",
    "train_t_un = 0.1 * train_t_u\n",
    "val_t_un = 0.1 * val_t_u\n",
    "\n",
    "params = torch.tensor([1.0, 0.0], requires_grad=True)\n",
    "learning_rate = 1e-2\n",
    "optimizer = optim.SGD([params], lr = learning_rate)\n",
    "params = training_loop2(n_epochs=5000, optimizer=optimizer, params=params, train_t_u= train_t_un, trina_t_c= train_t_c, val_t_u=val_t_un, val_t_c = val_t_c)\n",
    "print(params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
